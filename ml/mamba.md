https://github.com/Saivineeth147/awesome-llm-resources

https://github.com/mohamedrxo/simplegrad

https://www.gilesthomas.com/2025/10/llm-from-scratch-20-starting-training-cross-entropy-loss

https://stepik.org/course/231306/promo

https://habr.com/ru/articles/951428/ –ß—Ç–æ —Ç–∞–∫–æ–µ AI-–∞–≥–µ–Ω—Ç –∏ –∏–∑ –∫–∞–∫–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —á–∞—Å—Ç–µ–π –æ–Ω —Å–æ—Å—Ç–æ–∏—Ç

https://habr.com/ru/articles/951390/ AI-–¥–≤–∏–∂–∫–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ Knowledge Distillation, GAN, Reinforcement learning

https://huggingface.co/learn

https://ma-lab-berkeley.github.io/deep-representation-learning-book/ 

### Reinforcement learning: 
https://www.understandingai.org/p/reinforcement-learning-explained

https://habr.com/ru/articles/958062/

https://habr.com/ru/companies/otus/articles/951412/ Q-learning

https://arxiv.org/pdf/1905.03375  Embarrassingly Shallow Autoencoders for Sparse Data

## Casual Inference

https://matheusfacure.github.io/python-causality-handbook/landing-page.html
### Book
https://www.manning.com/books/deep-learning-with-python-third-edition 

### Book with code: Understanding Deep Learning 
https://udlbook.github.io/udlbook/

Book:
https://www.dataschool.io/master-machine-learning-book-preview/

###  Sebastian Rashka

https://sebastianraschka.com/

https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch01/#table-of-contents

https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch02/

https://habr.com/ru/articles/926160/  Russian translation of sebastianraschka.com

https://habr.com/ru/articles/958880/

https://sebastianraschka.com/notebooks/ml-notebooks/

###  Book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 3rd Edition

https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975/ 

### Book: https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html

### Book: Forecasting: Principles and Practice, the Pythonic Way
https://otexts.com/fpppy

### Book: Information Theory, Inference, and Learning Algorithms. David J.C. MacKay
https://www.inference.org.uk/itprnn/book.pdf


–û—Ç –ø—Ä–æ–º—Ç–æ–≤ –∫ –∞–≥–µ–Ω—Ç–∞–º: –∫–∞–∫ –º—ã –¥–æ—à–ª–∏ –¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —á—Ç–æ LLM —É–º–µ—é—Ç —É–∂–µ —Å–µ–π—á–∞—Å –∏ —á—Ç–æ –Ω–∞—Å –∂–¥—ë—Ç –≤ 2027 –≥–æ–¥—É
https://habr.com/ru/companies/netologyru/articles/926776/

–ê–Ω—Ç–æ–Ω –ë–æ–π—Ü–µ–≤
https://www.youtube.com/@%D0%90%D0%BD%D1%82%D0%BE%D0%BD%D0%91%D0%BE%D0%B9%D1%86%D0%B5%D0%B2-%D1%8C9%D0%BE/videos

https://github.com/Dyakonov/DL

https://habr.com/ru/companies/pgk/articles/921596/ –ü—Ä–æ–∫–∞—á–∞—Ç—å—Å—è –≤ data science: –±–ª–æ–≥–∏ –∏ –∫–∞–Ω–∞–ª—ã

https://habr.com/ru/articles/926398/  ML interview preparation

https://www.youtube.com/watch?v=pdNYw6qwuNc 
What Are Neural Networks Even Doing? (Manifold Hypothesis)

https://dongou.tech/ai/dongou/ai-by-hand-%E2%9C%8D%EF%B8%8F-with-prof-tom-yeh-for-ai-professionals/

https://habr.com/ru/articles/918188/ –ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–æ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π

https://github.com/gavinkhung/machine-learning-visualized   ML visualized

https://habr.com/ru/articles/918438/ –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ ML/DL

https://habr.com/ru/articles/955636/ –¢–æ–ø –≤–æ–ø—Ä–æ—Å–æ–≤ —Å Data Science —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–π

https://habr.com/ru/articles/926398/

https://habr.com/ru/articles/926160/
 
https://datasecrets.ru/articles

### https://franknielsen.github.io/Books/CuratedBookLists.html

https://habr.com/ru/articles/917664/ –æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π –º–∏—Ä–∞ ML

### Self-attention implementation

https://eli.thegreenplace.net/2025/notes-on-implementing-attention/

https://news.ycombinator.com/item?id=44276476 I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch (github.com/yousef-rafat)

### Transformers

https://rti.github.io/gptvis/ Understanding Transformers Using A Minimal Example  
https://news.ycombinator.com/item?id=45116957

https://nlp.seas.harvard.edu/annotated-transformer/

https://habr.com/ru/articles/925404/ Text-to-LoRA: –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤

https://vectorfold.studio/blog/transformers

https://www.stephendiehl.com/posts/post_transformers/  Attention ?

https://github.com/ageron/handson-ml3



https://machine-learning-with-python.readthedocs.io/

https://lux-api.readthedocs.io/en/latest/index.html



https://eli.thegreenplace.net/2025/notes-on-implementing-attention/

https://eli.thegreenplace.net/2025/convolutions-polynomials-and-flipped-kernels/

https://news.ycombinator.com/item?id=44048306

https://habr.com/ru/companies/yandex_praktikum/articles/901432/

https://www.kaggle.com/discussions/getting-started/390402

## AI Chip startups

Cerebras Systems

Tenstorrent

SiMa.ai  

https://groq.com/ 

Lightmatter


### Ads clicks prediction

https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLSD/mlsd-ads-ranking.md

### ML

https://github.com/HandsOnLLM/Hands-On-Large-Language-Models

https://news.ycombinator.com/item?id=43586073

https://habr.com/ru/articles/895332/ Bayes

https://habr.com/ru/companies/alfa/articles/895002/ uplift

https://habr.com/ru/companies/wunderfund/articles/894100/



https://arxiv.org/abs/2206.13446 Pen and Paper Exercises in Machine Learning

https://riverml.xyz/latest/ Online machine learning in Python

https://www.kdnuggets.com/10-github-repositories-to-master-machine-learning

https://github.com/Coder-World04/Data-and-ML-Projects-

https://habr.com/ru/articles/795251/ –¢–∏–ø–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞. –ß–∞—Å—Ç—å 2. –ê –µ—Å—Ç—å –ª–∏ —Ç—Ä–µ–Ω–¥?

https://habr.com/ru/articles/795785/ 

https://habr.com/ru/articles/897946/ Error backpropagation

https://eli.thegreenplace.net/2025/reproducing-word2vec-with-jax/

https://habr.com/ru/companies/yadro/articles/896362/  Convolution –°–≤–µ—Ä—Ç–∫–∞ 

https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/

https://news.ycombinator.com/item?id=45110311 The maths you need to start understanding LLMs

### Probabilistic programming with NumPy powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.
https://news.ycombinator.com/item?id=42156126    
https://num.pyro.ai/en/stable/   
https://github.com/pyro-ppl/numpyro    




### Feature store
https://asrathore08.medium.com/feature-store-architecture-1324eff5a573

https://github.com/alteryx/featuretools Feature Engineering

### Comp vision

https://habr.com/ru/articles/908168/

### Mamba
https://habr.com/ru/articles/786278/

https://habr.com/ru/articles/925416/


https://bernsteinbear.com/blog/simple-search/   Search 

### Multimodal LLM
https://arxiv.org/abs/2306.13549

Multimodal deep learning
https://arxiv.org/abs/2301.04856 

–°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω—ã—Ö AI-–∞–≥–µ–Ω—Ç–æ–≤: –ø–æ–ª–Ω—ã–π –∫—É—Ä—Å –ø–æ LangGraph –æ—Ç –ê –¥–æ –Ø. –ß–∞—Å—Ç—å 2. –î–∏–∞–ª–æ–≥–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã: –ø–∞–º—è—Ç—å, —Å–æ–æ–±—â–µ–Ω–∏—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
https://habr.com/ru/companies/amvera/articles/948000/

### AIC (Akaike Information Criterion)  BIC ( Bayesian information criterion )
```
What is AIC and BIC? How these helps in ML Model Selection? 
The AIC can be used to select between the additive and multiplicative Holt-Winters models.
 Bayesian information criterion (BIC) (Stone, 1979) is another criteria for model selection
that measures the trade-off between model fit and complexity of the model.
A lower AIC or BIC value indicates a better fit.

AIC criterion often risk choosing too large a model, whereas BIC often risk choosing too small a model.
In modelling, there's always a risk of either under-fitting, for small n or over-fitting for large n.

Have you chosen the best model?

You may want to check AIC and BIC.
Let's explore what they are and how they can help in finding the optimal ARIMA model üßµüëá
AIC and BIC are both model selection criteria used to compare and rank different models.
They help you choose the best model for your data by evaluating
the trade-off between the fit of the model and its complexity.

1Ô∏è‚É£ AIC (Akaike Information Criterion):
It is used to evaluate the quality of a model by penalizing the number of parameters in the model.
The AIC score is calculated as the negative log-likelihood of the observed data, a
djusted by the number of parameters in the model.
The model with the lowest AIC score is generally considered to be the best model for the given data.
‚≠ê This score is a good balance between the fit of the model and its complexity,
making it a popular choice for model selection.

2Ô∏è‚É£ BIC (Bayesian Information Criterion):
It's similar to AIC, but it penalizes the number of parameters in the model more heavily.
BIC score is calculated as the negative log-likelihood of the observed data,
adjusted by the number of parameters in the model and the sample size.
The BIC score is intended to balance the fit of the model to the data with the complexity of the model.
‚≠ê It's often used when the sample size is large, and you want to avoid overfitting.
So, when choosing between models, you can use either AIC or BIC to help you find the best one.
```
MLFlow https://habr.com/ru/companies/pgk/articles/904078/
