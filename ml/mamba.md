Reinforcement learning: https://www.understandingai.org/p/reinforcement-learning-explained

Book:
https://sebastianraschka.com/books/ml-q-and-ai-chapters/ch01/#table-of-contents

https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLSD/ml-system-design.md

https://github.com/Dyakonov/DL

https://habr.com/ru/companies/pgk/articles/921596/ –ü—Ä–æ–∫–∞—á–∞—Ç—å—Å—è –≤ data science: –±–ª–æ–≥–∏ –∏ –∫–∞–Ω–∞–ª—ã

https://www.youtube.com/watch?v=pdNYw6qwuNc 
What Are Neural Networks Even Doing? (Manifold Hypothesis)

https://dongou.tech/ai/dongou/ai-by-hand-%E2%9C%8D%EF%B8%8F-with-prof-tom-yeh-for-ai-professionals/

https://habr.com/ru/articles/918188/ –ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–æ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π

https://habr.com/ru/articles/918438/ –§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ ML/DL,
 
https://datasecrets.ru/articles

https://franknielsen.github.io/Books/CuratedBookLists.html

https://habr.com/ru/articles/917664/ –æ—Å–Ω–æ–≤–æ–ø–æ–ª–∞–≥–∞—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π –º–∏—Ä–∞ ML

### Book: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 3rd Edition

https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975/ 

https://github.com/ageron/handson-ml3

Book with code: Understanding Deep Learning 
https://udlbook.github.io/udlbook/

https://machine-learning-with-python.readthedocs.io/

https://lux-api.readthedocs.io/en/latest/index.html

https://sebastianraschka.com/notebooks/ml-notebooks/

https://eli.thegreenplace.net/2025/notes-on-implementing-attention/

https://eli.thegreenplace.net/2025/convolutions-polynomials-and-flipped-kernels/

https://news.ycombinator.com/item?id=44048306

https://habr.com/ru/companies/yandex_praktikum/articles/901432/

https://www.kaggle.com/discussions/getting-started/390402

### ML System design 

https://www.youtube.com/watch?v=iqbsHiSnZQE System Design —Å –í–∞–ª–µ—Ä–∏–µ–º –ë–∞–±—É—à–∫–∏–Ω—ã–º 

https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLSD/ml-system-design.md

https://habr.com/ru/articles/900788/  ML project setup

https://habr.com/ru/companies/ingos_it/articles/901782/  ML project setup

https://www.stephendiehl.com/posts/post_transformers/  Attention ?


### Ads clicks prediction

https://github.com/alirezadir/Machine-Learning-Interviews/blob/main/src/MLSD/mlsd-ads-ranking.md

### ML

Book: https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html

https://github.com/HandsOnLLM/Hands-On-Large-Language-Models

https://news.ycombinator.com/item?id=43586073

https://habr.com/ru/articles/895332/ Bayes

https://habr.com/ru/companies/alfa/articles/895002/ uplift

https://habr.com/ru/companies/wunderfund/articles/894100/

https://vectorfold.studio/blog/transformers

https://arxiv.org/abs/2206.13446 Pen and Paper Exercises in Machine Learning

https://riverml.xyz/latest/ Online machine learning in Python

https://www.kdnuggets.com/10-github-repositories-to-master-machine-learning

https://github.com/Coder-World04/Data-and-ML-Projects-

https://habr.com/ru/articles/795251/ –¢–∏–ø–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞. –ß–∞—Å—Ç—å 2. –ê –µ—Å—Ç—å –ª–∏ —Ç—Ä–µ–Ω–¥?

https://habr.com/ru/articles/795785/ 

https://habr.com/ru/articles/897946/ Error backpropagation

https://eli.thegreenplace.net/2025/reproducing-word2vec-with-jax/

https://habr.com/ru/companies/yadro/articles/896362/  –°–≤–µ—Ä—Ç–∫–∞ 


### ML
https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/



 

### Probabilistic programming with NumPy powered by JAX for autograd and JIT compilation to GPU/TPU/CPU.
https://news.ycombinator.com/item?id=42156126  
https://num.pyro.ai/en/stable/ 
https://github.com/pyro-ppl/numpyro  




### Self-attention implementation

https://eli.thegreenplace.net/2025/notes-on-implementing-attention/

https://news.ycombinator.com/item?id=44276476 I have reimplemented Stable Diffusion 3.5 from scratch in pure PyTorch (github.com/yousef-rafat)

### Automatic differentiation

https://huggingface.co/blog/andmholm/what-is-automatic-differentiation

https://habr.com/ru/articles/874592/

https://eli.thegreenplace.net/2025/reverse-mode-automatic-differentiation/

MLFlow https://habr.com/ru/companies/pgk/articles/904078/

### Outliers and Anomaly  detection

https://leftjoin.ru/blog/data-analysis/outliers-detection-in-python/

https://talkpython.fm/episodes/show/497/outlier-detection-with-python

–∞–ª–≥–æ—Ä–∏—Ç–º—ã HBOS –∏ ECOD,  –∏ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ PyOD: 

https://habr.com/ru/companies/garda/articles/895148/

https://medium.com/pythoneers/hidden-gems-in-outlier-detection-3-powerful-lesser-known-methods-4386ac9ecff7 

https://www.datasciencecentral.com/11-articles-and-tutorials-about-outliers/


https://github.com/alteryx/featuretools Feature Engineering

### Comp vision

https://habr.com/ru/articles/908168/

### Mamba
https://habr.com/ru/articles/786278/

https://habr.com/ru/articles/925416/


https://bernsteinbear.com/blog/simple-search/   Search 

### Multimodal LLM
https://arxiv.org/abs/2306.13549

Multimodal deep learning
https://arxiv.org/abs/2301.04856 
```
What is AIC and BIC? How these helps in ML Model Selection? 
The AIC can be used to select between the additive and multiplicative Holt-Winters models.
 Bayesian information criterion (BIC) (Stone, 1979) is another criteria for model selection
that measures the trade-off between model fit and complexity of the model.
A lower AIC or BIC value indicates a better fit.

AIC criterion often risk choosing too large a model, whereas BIC often risk choosing too small a model.
In modelling, there's always a risk of either under-fitting, for small n or over-fitting for large n.

Have you chosen the best model?

You may want to check AIC and BIC.
Let's explore what they are and how they can help in finding the optimal ARIMA model üßµüëá
AIC and BIC are both model selection criteria used to compare and rank different models.
They help you choose the best model for your data by evaluating
the trade-off between the fit of the model and its complexity.
1Ô∏è‚É£ AIC (Akaike Information Criterion):
It is used to evaluate the quality of a model by penalizing the number of parameters in the model.
The AIC score is calculated as the negative log-likelihood of the observed data, a
djusted by the number of parameters in the model.
The model with the lowest AIC score is generally considered to be the best model for the given data.
‚≠ê This score is a good balance between the fit of the model and its complexity,
making it a popular choice for model selection.
2Ô∏è‚É£ BIC (Bayesian Information Criterion):
It's similar to AIC, but it penalizes the number of parameters in the model more heavily.
BIC score is calculated as the negative log-likelihood of the observed data,
adjusted by the number of parameters in the model and the sample size.
The BIC score is intended to balance the fit of the model to the data with the complexity of the model.
‚≠ê It's often used when the sample size is large, and you want to avoid overfitting.
So, when choosing between models, you can use either AIC or BIC to help you find the best one.
```
