1) Explain the different file formats that you are using in your project (CSV, JSON, PARQUET, Delta Table)?
2) If in source we have three columns and in destination we want to add one more column then how can you achieve this in ADF?
3) How did you implemented SCD2 in your project?
4) How to transfer data to azure synapse?
5) What is the significance of Catalyst Optimizer in PYSPARK?
6) What is schema on read and schema on write?
7) How much data you deal with on daily basic?
8) Explain the out of memory issue?
9) Query to delete the duplicate records? Explain all other types to delete duplicate records in SQL?
10) Explain Windows functions with example in SQL and PYSPARK?
11) What is serverless computing?
12) If a job is failed then how did you debug it?
13) How to optimize the slow running query?
14) What is shuffling in transformations?
15) How to use nested JSON with data bricks?
16) If data is not loading from the source in Azure Data Factory (ADF), then what will you do?
17) How many pipelines you have created?
18) PYSPARK command to read the data from a file into a data frame?
19) What is explode function in PYSPARK?
20) Write a code to read a parquet file?
21) What is YARN?
22) How did you create the Temp View?
23) Diff between DAG and LINEAGE?
24) How did you manage the pipeline from failed activity?
25) What is Partition? how spark partitions the data?
26) What is broadcast variable and broadcast joins in spark?
27) What is serialization and deserialization?
28) What is Common Data Model (CDM)?
29) What are the steps you have taken for data security in your project?
30) Diff between procedure and functions?
31) What is normalization and denormalization? what are its uses?
32) Diff between data lake and delta lake and data warehouse? 
33) What is time travel? how did you use time travel in your project?
34) How does Z Ordering works? What is the use of Z ordering?
35) How did you get the data from Rest API?
36) What is data skewness in spark?
37) Merge statement in data bricks?
38) What are the prerequisites before the migration?
39) What is logic apps? How did you used logic apps in your project?
40) Diff between coalesce and is null?


🔹 𝗥𝗼𝘂𝗻𝗱 𝟭: 𝗦𝗤𝗟 + 𝗗𝗮𝘁𝗮 𝗖𝗮𝘀𝗲 𝗦𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀 (𝗠𝗲𝗱𝗶𝘂𝗺)
Questions I got:

1. “Write a query to get users who made purchases 3 months in a row.”
2. “Find duplicate transactions but only return the latest one per user.”
3. “Explain how you’d optimize a query that joins 3 tables with billions of rows.”
4. They weren’t testing syntax. They were testing how I approach real data.

🔹 𝗥𝗼𝘂𝗻𝗱 𝟮: 𝗣𝘆𝘁𝗵𝗼𝗻 + 𝗗𝗮𝘁𝗮 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻𝘀 (𝗠𝗲𝗱𝗶𝘂𝗺-𝗛𝗮𝗿𝗱)
Questions I got:

 4. “Write a Python script to detect missing data in time-series logs.”
 5. “How would you handle large file ingestion with memory constraints?”
 6. “Build a lightweight validation framework before loading to the warehouse.”

🔹 𝗥𝗼𝘂𝗻𝗱 𝟯: 𝗦𝘆𝘀𝘁𝗲𝗺 𝗗𝗲𝘀𝗶𝗴𝗻 𝗳𝗼𝗿 𝗗𝗮𝘁𝗮 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲𝘀 (𝗛𝗮𝗿𝗱)
Questions I got:

 7. “Design an end-to-end pipeline for processing clickstream data from millions of users.”
 8. “How do you make your pipeline idempotent and replay-safe?”
 9. “What would you do if your batch job is delayed and downstream dashboards break?”
Here, I had to think in systems, not just scripts.

🔹 𝗥𝗼𝘂𝗻𝗱 𝟰: 𝗦𝗽𝗮𝗿𝗸, 𝗞𝗮𝗳𝗸𝗮 & 𝗥𝗲𝗮𝗹-𝗧𝗶𝗺𝗲 𝗣𝗿𝗼𝗰𝗲𝘀𝘀𝗶𝗻𝗴 (𝗛𝗮𝗿𝗱)
Questions I got:

 10. “What causes shuffle in Spark? How can you avoid it?”
 11. “How do you manage Kafka lag in a consumer group that can’t scale further?”
 12. “Design a fault-tolerant stream processing pipeline with exactly-once semantics.”

✅ 𝗧𝗵𝗲 𝗧𝘂𝗿𝗻𝗶𝗻𝗴 𝗣𝗼𝗶𝗻𝘁?
In my final interview, I was asked:
“What’s one pipeline you built that failed—and how did you fix it?”


