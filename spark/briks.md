### Databricks Versions 
| Version            | Spark Version | Release Date   | Support Ends                                                     |
| ------------------ | ------------- | -------------- | ---------------------------------------------------------------- |
| **16.4‚ÄØLTS**       | 3.5.2         | May 9,‚ÄØ2025    | May 9,‚ÄØ2028                                                      |
| 15.4‚ÄØLTS           | 3.5.0         | Aug 19,‚ÄØ2024   | Aug 19,‚ÄØ2027                                                     |
| 14.3‚ÄØLTS           | 3.5.0         | Feb 1,‚ÄØ2024    | Feb 1,‚ÄØ2027                                                      |
| ‚Ä¶ Other LTS        | Older Sparks  | Various        | See release notes ([docs.databricks.com][1], [docs.azure.cn][2]) |
| **17.0 (Beta)**    | 4.0.0         | May 20,‚ÄØ2025   | Nov 20,‚ÄØ2025                                                     |
| 16.3 / 16.2 / 16.1 | 3.5.x         | Early‚Äìmid 2025 | Mid‚Äìlate 2025                                                    |


### Delta Lake
Delta Lake is an open-source storage layer that brings reliability, performance, and governance to data lakes. It is tightly integrated with Databricks and underpins much of its functionality.

‚úÖ Key Features:
ACID Transactions: Guarantees data integrity with support for concurrent writes and reads.

Schema Enforcement & Evolution: Prevents bad data and allows schema changes over time.

Time Travel: Query historical versions of data using VERSION AS OF or TIMESTAMP AS OF.

Data Compaction (OPTIMIZE): Merges small files into larger ones to improve performance.

Streaming + Batch: Supports both in the same pipeline using the same Delta table.

### Liquid clustering

Liquid Clustering in Databricks is a data layout optimization feature for Delta Lake tables,   
designed to improve query performance and simplify maintenance compared to traditional Z-Ordering.   
As of 2025, liquid clustering is not obsolete‚Äîin fact, it‚Äôs the preferred and modern method recommended by Databricks for clustering large Delta tables.

Liquid Clustering automatically maintains clustering of data files on one or more columns   
(like customer_id, event_date, etc.) without requiring static Z-Ordering.   
It uses lightweight background operations to organize data incrementally, making it suitable for large and frequently updated tables.

| Feature                                 | Description                                                          |
| --------------------------------------- | -------------------------------------------------------------------- |
| **Incremental optimization**            | Files are re-clustered automatically in the background during writes |
| **No full rewrites**                    | Unlike Z-Order, it avoids full rewrites or expensive compactions     |
| **Adaptive to changes**                 | Adjusts clustering as new data arrives                               |
| **Simpler maintenance**                 | No need for regular OPTIMIZE ZORDER BY jobs                          |
| **Better for streaming/near real-time** | Works well with continuous data ingestion scenarios                  |

Use Liquid Clustering when:

You have large tables with frequent updates/inserts

Query performance suffers due to data skew or file fragmentation

You need a less maintenance-heavy alternative to Z-Ordering

```sql
ALTER TABLE my_table
SET TBLPROPERTIES (
  'delta.liquidClustered.columns' = 'event_date, user_id'
);

or

CREATE TABLE my_table (
  event_date DATE,
  user_id STRING,
  ...
)
USING DELTA
TBLPROPERTIES (
  'delta.liquidClustered.columns' = 'event_date, user_id'
);
```
Databricks is moving away from recommending Z-Ordering for many use cases.

Liquid Clustering is better aligned with streaming + batch hybrid pipelines.

| Feature     | Z-Ordering                      | Liquid Clustering              |
| ----------- | ------------------------------- | ------------------------------ |
| Maintenance | Manual `OPTIMIZE ZORDER BY`     | Automatic                      |
| Performance | High, but expensive to maintain | Comparable, and auto-adjusting |
| Best For    | Static/batch workloads          | Streaming + frequent updates   |
| Obsolete?   | No, but becoming less preferred | No, recommended for modern use |


### Clustering (Z-Ordering)
Clustering in Delta Lake is done using Z-Ordering, a technique to optimize data layout on disk for faster query performance.

‚úÖ Key Concepts:
Z-Ordering: Reorders data files on disk to colocate related records (e.g., by customer_id or timestamp).

Improves predicate pushdown and data skipping, so queries scan fewer files.

Used with OPTIMIZE command:

```sql
OPTIMIZE table_name ZORDER BY (col1, col2);
```
üîç When to Use:
Large tables where you frequently query by filters on certain columns (e.g., date ranges, user IDs).

Example: speeding up queries on a logs table by Z-Ordering on event_time.



### Table properties
```sql
CREATE TABLE sales_data (
    sale_id STRING,
    customer_id STRING,
    amount DOUBLE,
    sale_date DATE
)
USING DELTA
TBLPROPERTIES (
    'delta.autoOptimize.optimizeWrite' = 'true',
    'delta.autoOptimize.autoCompact' = 'true',
    'comment' = 'Sales transactions table',
    'source' = 'ETL_v2'
);

TBLPROPERTIES ('source' = 'ingestion_pipeline', 'pii' = 'true')


DESCRIBE TABLE EXTENDED sales_data;

SHOW TBLPROPERTIES sales_data;

```

#### Delta Lake-Specific Properties
 
| Property                                                    | Description                                                                     |
| ----------------------------------------------------------- | ------------------------------------------------------------------------------- |
| `delta.autoOptimize.optimizeWrite = true`                   | Automatically optimizes file sizes when writing data.                           |
| `delta.autoOptimize.autoCompact = true`                     | Automatically compacts small files after write operations.                      |
| `delta.logRetentionDuration = 'interval'`                   | Retain Delta transaction log files for a specific duration (e.g., `'30 days'`). |
| `delta.deletedFileRetentionDuration = 'interval'`           | Controls how long deleted files are retained (for time travel).                 |
| `delta.enableChangeDataFeed = true`                         | Enables Change Data Feed (CDF) for the table.                                   |
| `delta.columnMapping.mode = 'name'`                         | Enables column mapping by name, useful for schema evolution.                    |
| `delta.minReaderVersion = X` / `delta.minWriterVersion = Y` | Sets required Delta protocol versions.                                          |

#### Clustering and Performance

| Property                           | Description                                     |
| ---------------------------------- | ----------------------------------------------- |
| `delta.dataSkippingNumIndexedCols` | Number of columns used for data skipping.       |
| `delta.checkConstraints.<name>`    | Used to enforce a check constraint on a column. |
| `delta.zOrderCols` *(internal)*    | Metadata about columns used for Z-ordering.     |

#### Access Control

| Property  | Description                                                   |
| --------- | ------------------------------------------------------------- |
| `owner`   | Sets the owner of the table (for Unity Catalog).              |
| `comment` | Adds a description to the table. Useful for catalog browsing. |




### DLT Delta Live Tables

DLT is a declarative ETL framework built into Databricks that:
 ‚Ä¢ Automates task orchestration
 ‚Ä¢ Manages clusters and errors
 ‚Ä¢ Integrates data quality checks
 ‚Ä¢ Supports streaming + batch workloads


üìä Types of Datasets in DLT

DLT supports three main dataset types, each designed for specific pipeline needs:  
 ‚Ä¢ üîÑ Streaming Table:  
Processes data in real-time (append-only). Ideal for low-latency ingestion and continuous data flow.
 ‚Ä¢ üíæ Materialized View:  
Stores precomputed results in a Delta table. Great for aggregations, CDC, or frequently accessed data.
 ‚Ä¢ üëì View:  
Logical, on-demand computation used for intermediate transformations and data quality validation.

üîÅ Simplified CDC with APPLY CHANGES

DLT removes the pain of handling Change Data Capture:  
 ‚Ä¢ Automatically handles late-arriving records  
 ‚Ä¢ Eliminates complex merge/update logic  
 ‚Ä¢ Ensures accuracy and consistency in target tables  


üì• Streamlined Data Ingestion

DLT supports ingestion from:
 ‚Ä¢ Cloud storage (S3, ADLS, GCS)
 ‚Ä¢ Kafka/message queues
 ‚Ä¢ Databases like PostgreSQL

üí° Pro Tip: Use Auto Loader + Streaming Tables for optimized performance!

‚úÖ Built-in Data Quality with Expectations

Define rules to validate data as it flows:
 ‚Ä¢ EXPECT <condition> ‚Äì log and continue
 ‚Ä¢ EXPECT ... ON VIOLATION DROP ‚Äì discard bad records
 ‚Ä¢ EXPECT ... ON VIOLATION FAIL ‚Äì halt pipeline on error

These expectations give you robust data governance with real-time metrics.


üí° Why It Matters:
 ‚Ä¢ üîπ Declarative + testable pipelines
 ‚Ä¢ üîπ Fully managed orchestration
 ‚Ä¢ üîπ Native support for streaming, CDC, batch
 ‚Ä¢ üîπ Built-in quality checks and lineage tracking



### Unity Catalog
Unity Catalog is Databricks‚Äô unified governance layer for data, ML models, and notebooks across all workspaces and cloud providers.

‚úÖ Key Features:
Centralized Metadata Management: One place to manage schemas, tables, and permissions.

Fine-Grained Access Control:

Table-, row-, and column-level security using RBAC.

Supports attribute-based access control (ABAC) with dynamic views.

Data Lineage Tracking:

Automatically captures full lineage for tables, views, and notebooks.

Governance Across Workspaces:

Share assets across multiple Databricks workspaces securely.

üõ°Ô∏è Example Benefits:
Regulatory compliance (e.g., GDPR, HIPAA).

Centralized control of who can see and edit specific data.

Secure data sharing across teams or business units.

