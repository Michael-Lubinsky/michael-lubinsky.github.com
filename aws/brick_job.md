How to run your PySpark code every minute **outside a notebook** and manage it end-to-end.

# 1) Package your notebook logic as a Python entrypoint

Refactor your notebook into a tiny callable module plus a `main.py` runner. Put this in a Databricks Repo (recommended) or upload to DBFS.

```
repo-root/
  ingest/
    __init__.py
    job_logic.py      # your existing function(s)
    main.py           # entrypoint that calls your function
```

`job_logic.py` (sketch)

```
from pyspark.sql import SparkSession

def run_job():
    spark = SparkSession.builder.getOrCreate()

    # 1) Read from DynamoDB (however you currently do it)
    # ddb = spark.read.format("dynamodb")...  (or your connector)
    # 2) Transform
    # 3) Write to your Databricks table (on S3 / Delta)

    # example:
    # df.write.format("delta").mode("append").saveAsTable("my_catalog.my_schema.my_table")
```

`main.py`

```
from ingest.job_logic import run_job

if __name__ == "__main__":
    run_job()
```

Push this repo to Git and use **Repos** in Databricks to keep it synced.

# 2) Create a Databricks Job (non-interactive)

You have three good compute choices:

* **Serverless compute for Jobs** (best if available): fully managed, quick starts, no cluster babysitting.
* **Job cluster**: ephemeral cluster created per run (simple, but cold-start overhead every minute can be costly).
* **Existing all-purpose cluster**: fastest per-minute cadence (no cold start), but you pay for it to stay running.

For a true **every-minute** cadence, prefer **Serverless** or an **Existing cluster kept warm**. A Job cluster spinning up every minute is usually too slow/expensive.

## Create the Job (UI)

* Workflows ‚Üí Jobs ‚Üí Create Job
* Task type: **Python script**
* Source: **Repo** ‚Üí pick your `main.py`
* Compute:

  * **Use Serverless compute** (if your workspace supports it), or
  * **Use existing cluster** (kept running; set Auto-Terminate to something long), or
  * **New job cluster** (only if you can tolerate startup time)
* Schedule: **Every 1 minute** (Scheduler uses UTC)
* Set **Timeout** (e.g., 50 seconds) and **Max concurrent runs = 1** to avoid overlaps
* Add **Retry** policy (e.g., 3 attempts, 10s backoff)
* Save

## Create the Job (JSON + CLI)

If you automate with the new `databricks` CLI:

`job.json` (example; adjust paths)

```
{
  "name": "ddb-to-delta-every-minute",
  "max_concurrent_runs": 1,
  "timeout_seconds": 50,
  "tasks": [
    {
      "task_key": "ingest-task",
      "python_wheel_task": null,
      "spark_python_task": {
        "python_file": "repos/you/your-repo/ingest/main.py"
      },
      "job_cluster_key": "job-cluster",
      "timeout_seconds": 50,
      "max_retries": 3,
      "min_retry_interval_millis": 10000,
      "retry_on_timeout": true
    }
  ],
  "job_clusters": [
    {
      "job_cluster_key": "job-cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.xlarge",
        "num_workers": 1
      }
    }
  ],
  "schedule": {
    "quartz_cron_expression": "0 0/1 * * * ?",
    "timezone_id": "UTC",
    "pause_status": "UNPAUSED"
  }
}
```

Create/update:

```
databricks jobs create --json-file job.json
# or
databricks jobs reset --job-id <id> --json-file job.json
```

**Tip:** In the UI, selecting ‚ÄúEvery minute‚Äù will populate a correct Quartz cron; if you‚Äôre unsure about the expression, use the UI first.

# 3) Credentials to DynamoDB/S3

Because you‚Äôre on **Serverless/Jobs**, the clean path is an **instance profile / IAM role** that grants read to DynamoDB and write to your S3/Delta location. In the job‚Äôs compute settings:

* For **Serverless compute**: attach the workspace‚Äôs **serverless access**/instance profile (your admin might need to map it).
* For **Existing or Job clusters**: set the **Instance Profile / IAM role**.
  Then your Spark code can use the default provider chain (no hardcoded keys). In Python, `boto3` (if used) will pick up the role automatically.

# 4) If you truly need ‚Äúevery minute,‚Äù consider streaming

A Structured Streaming job is often better than cron-style runs. You can:

* Wrap your logic as a streaming pipeline (e.g., `Trigger.ProcessingTime("1 minute")`).
* Deploy as a **continuous** job (no schedule; it runs once and keeps going).
* Use **Serverless** or an **existing cluster** to keep it alive.
  This avoids per-run startup, improves latency, and handles backpressure.

Minimal pattern:

```
(
  spark.readStream.format("...dynamodb-source...")
  .load()
  # transforms
  .writeStream
  .format("delta")
  .option("checkpointLocation", "s3://‚Ä¶/chk/ingest-ddb")
  .trigger(processingTime="1 minute")
  .toTable("my_catalog.my_schema.my_table")
)
```

# 5) How to stop the non-interactive job

You have several options:

* **Cancel the active run:** Workflows ‚Üí Runs ‚Üí select run ‚Üí **Cancel**
  CLI: `databricks runs cancel --run-id <run-id>`
* **Pause the schedule:** Workflows ‚Üí Job ‚Üí **Pause**
  CLI: `databricks jobs pause --job-id <job-id>`
* **Delete the job:** Workflows ‚Üí Job ‚Üí **Delete**
  CLI: `databricks jobs delete --job-id <job-id>`
* **Stop compute:**

  * Serverless: stopping the run is enough; there‚Äôs no cluster to manage.
  * Existing cluster: **Terminate** it in Compute if you used one for the job.

# 6) Quick checklist for your case

* [ ] Move your notebook logic into `job_logic.run_job()`
* [ ] Put code in a Repo (so Jobs ‚Üí Python script can point to it)
* [ ] Choose **Serverless compute for Jobs** (or an existing warm cluster)
* [ ] Schedule ‚ÄúEvery minute‚Äù (UTC), timeout < 60s, retries, max concurrent = 1
* [ ] Ensure IAM role/instance profile grants DynamoDB read + S3 write
* [ ] (Optional but recommended) Convert to a single **streaming** job instead of per-minute batch

 
  **Databricks notebook scheduling (via the UI)** only supports hourly, daily, or weekly intervals ‚Äî not per-minute execution.

# How to achieve **every-minute runs** :  several  patterns  
 

## üß≠ 1. Use a Databricks **Job** with a cron schedule

Databricks **Jobs** (not the simple ‚ÄúSchedule‚Äù in a notebook) can use **custom cron expressions**.

**Steps:**

1. Open **Workflows ‚Üí Jobs** in the left sidebar.
2. Click **Create Job**.
3. Under **Task**, select your notebook.
4. In **Schedule**, click **Advanced** and enter this cron expression:

   ```
   * * * * *    ‚Üí every minute
   ```
5. Save and run.

‚úÖ Runs every minute
‚úÖ Fully managed by Databricks
‚úÖ Viewable in the Job Runs history
‚ö†Ô∏è But: a new cluster may start for each run ‚Äî use a **‚ÄúContinuous job‚Äù cluster** or **existing job cluster** to minimize overhead.

---

## ‚öôÔ∏è 2. Use **Databricks REST API** to trigger the job every minute

You can have an external scheduler (like Azure Function, AWS Lambda, or a local cron job) call:

```
POST /api/2.1/jobs/run-now
```

Example Python script:

```python
import time
import requests

DATABRICKS_INSTANCE = "https://<your-instance>.cloud.databricks.com"
JOB_ID = "<your-job-id>"
TOKEN = "<your-personal-access-token>"

while True:
    response = requests.post(
        f"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now",
        headers={"Authorization": f"Bearer {TOKEN}"},
        json={"job_id": JOB_ID}
    )
    print(response.json())
    time.sleep(60)  # wait 1 minute
```

‚úÖ Fine control over timing
‚úÖ Can handle retries or conditions
‚ö†Ô∏è Requires an always-on process somewhere

---

## üîÅ 3. Loop inside the notebook itself

You can make your notebook run continuously, executing the code every minute inside a loop:

```python
import time

while True:
    print("Running task...")
    # your PySpark or Python logic here
    time.sleep(60)
```

‚úÖ Simplest approach
‚ö†Ô∏è Runs forever ‚Äî must keep the notebook/cluster alive manually
‚ö†Ô∏è No job-level monitoring or retries

---

## ü™∂ 4. Use **Databricks Workflows ‚Üí Continuous jobs (Streaming mode)**

If your code reads from streaming sources or can be adapted to a long-running pattern, you can enable **continuous execution**.
That‚Äôs ideal for incremental ingestion or minute-based aggregation.

In Job settings:

* Toggle **‚ÄúRun continuously‚Äù**
* The code runs non-stop instead of restarting every minute.

‚úÖ Ideal for event processing or streaming data
‚ö†Ô∏è Not good for batch-style jobs that must start/finish each time

---

## üß∞ 5. Alternative external schedulers

If Databricks isn‚Äôt strict about scheduling precision:

* Use **Azure Function (Timer Trigger)** every minute to call your Databricks job.
* Or use **Apache Airflow** DAG with `schedule_interval="@minutely"`.

---

### ‚úÖ Summary

| Method                    | Granularity  | Recommended for       | Notes                 |
| ------------------------- | ------------ | --------------------- | --------------------- |
| Databricks Job (cron)     | Every minute | Standard automation   | Use `* * * * *`       |
| REST API loop             | Every minute | Custom orchestration  | Flexible, external    |
| Infinite loop in notebook | Every minute | Quick tests           | No retries            |
| Continuous job            | Real-time    | Streaming tasks       | Keeps cluster running |
| External scheduler        | Every minute | Integration pipelines | Use Function/Airflow  |

---

There is DynamoDB table populated by external process.
Goal is to build almost real-time pipeline which 
reads the new records from DynamoDB, transform it  using PySpark and append transformed records into DataBricks Unity catalog table.

I consider following options:

Option 1:  Direct DynamoDB Table Access (Batch Reads)
---------------------------------------------------
Direct polling from DynamoDB table to Databricks using GSI global secondry index on column updated_at
Create Global Secondary Index (GSI) on updated_at column.
Periodically call table.query() 
Store max(updated_at) of already processed records in persistent storage, in order to read
only new recors:   max(updated_at) will be passed as argument to  tab.query() 


Option 2: DynamoDB Streams (Change Data Capture)
----------------------------------------------
AWS Lambda ->  S3 -> Databricks


 
Here‚Äôs a pragmatic comparison + a concrete reference design for a near-real-time (sub-minute to a few minutes) DynamoDB ‚Üí PySpark ‚Üí Unity Catalog pipeline. I‚Äôll keep it punchy and actionable.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- If you want ‚Äúrobust + low-ops + ~1‚Äì2 min latency‚Äù: **Use DynamoDB Streams ‚Üí (EventBridge Pipes or Kinesis Firehose) ‚Üí S3 ‚Üí Databricks Auto Loader ‚Üí Delta/Unity**. This is the sweet spot for cost, reliability, and simplicity.
- If volume is small and you need a quick start with minute polling: **Use a GSI on (tenant, updated_at)** and batch pull with a persisted cursor + idempotent MERGE into Delta.
- If you truly need sub-second to tens-of-seconds latency without an S3 landing zone: **Streams ‚Üí Kinesis Data Streams ‚Üí Databricks Structured Streaming**, but this adds complexity.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Option 1 ‚Äî Direct DynamoDB table access (batch pulls via GSI)
Good for: low/medium volume, simple ops, minute-level cadence.

Design
1) Model your GSI carefully:
   - **Partition key (HASH)**: something selective you can query per batch, e.g. tenant/account/org_id (or a fixed bucket key if global).
   - **Sort key (RANGE)**: updated_at (ISO-8601 or epoch millis).
   - If you have many tenants, you‚Äôll query per tenant; otherwise shard into a small set of buckets (e.g. pk = ‚Äúbucket#00..99‚Äù) so queries are bounded.

2) Pull pattern:
   - Keep a persistent cursor per (partition key) = **last_processed_updated_at** in a tiny Delta ‚Äústate‚Äù table.
   - Query GSI with `KeyConditionExpression: pk=:tenant AND updated_at BETWEEN :cursor AND :cursor + window`.
   - **Paginate** (1 MB limit/page) and **backoff** on throttling (RCU).
   - Allow **overlap** (e.g., re-read last 1‚Äì2 minutes) to tolerate clock skew & eventual consistency; dedupe with MERGE on primary keys.

3) Write pattern (idempotent):
   - Stage incoming micro-batch to a temp Delta table / view, then **MERGE** into your Unity table on (pk, sort_key) or a true unique id.

Pros
- Simple to reason about, no extra AWS infra required.
- Easy to backfill: just move the cursor back.

Cons & gotchas
- You must manage cursor, retries, and dedupe yourself.
- Hot partitions possible if updated_at is monotonically increasing and the GSI partition key is too coarse (watch RCU).
- Deletes and TTL expirations won‚Äôt be seen unless you model soft-deletes or do periodic reconciliations.

Code sketch (Databricks notebook, using your provided credential handle)
- Assumes your admin exposed AWS creds via `dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds")`.
- Replace `TENANT_IDS` with your partition keys or iterate all tenants you own.

from pyspark.sql import functions as F

# 1) Read state (last processed updated_at per tenant)
state_tbl = "hcai_databricks_dev.chargeminder2.ddb_state"   # tenant, last_ts
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {state_tbl} (
  tenant STRING,
  last_ts BIGINT
) USING DELTA
""")

def get_boto3():
    import boto3
    sess = boto3.Session(
        botocore_session=dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds"),
        region_name="us-east-1"
    )
    return sess.resource("dynamodb"), sess.client("dynamodb")

table_name = "your_ddb_table"
gsi_name   = "tenant_updated_at_gsi"   # HASH = tenant, RANGE = updated_at

TENANT_IDS = ["tenantA","tenantB"]     # or discover/maintain this list elsewhere
MAX_LAG_MS = 120000                    # re-read last 2 minutes for overlap

ddb_resource, ddb_client = get_boto3()
tbl = ddb_resource.Table(table_name)

import time, decimal
now_ms = int(time.time() * 1000)

rows = []
for tenant in TENANT_IDS:
    last = spark.table(state_tbl).where(F.col("tenant")==tenant).limit(1).collect()
    last_ts = last[0]["last_ts"] if last else (now_ms - 3600_000)  # default: 1 hour back for first run
    start_ts = max(0, last_ts - MAX_LAG_MS)
    kwargs = {
        "IndexName": gsi_name,
        "KeyConditionExpression": "#t = :tenant AND #u BETWEEN :from AND :to",
        "ExpressionAttributeNames": {"#t":"tenant", "#u":"updated_at"},
        "ExpressionAttributeValues": {":tenant": tenant, ":from": start_ts, ":to": now_ms},
        "Limit": 1000
    }
    while True:
        resp = tbl.query(**kwargs)
        for item in resp.get("Items", []):
            rows.append(item)
        if "LastEvaluatedKey" not in resp:
            break
        kwargs["ExclusiveStartKey"] = resp["LastEvaluatedKey"]

# Convert to Spark (handle Decimal)
def to_native(o):
    import decimal
    if isinstance(o, decimal.Decimal):
        return float(o)
    if isinstance(o, dict):
        return {k: to_native(v) for k,v in o.items()}
    if isinstance(o, list):
        return [to_native(v) for v in o]
    return o

clean = [to_native(r) for r in rows]
df = spark.createDataFrame(clean) if clean else spark.createDataFrame([], "tenant STRING")  # empty safe

# 2) Transform with PySpark
transformed = (
    df
    # .withColumn(...)  # your business transforms
)

# 3) Upsert into UC Delta table
target_tbl = "hcai_databricks_dev.chargeminder2.fact_events"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {target_tbl} (
  tenant STRING,
  id STRING,
  updated_at BIGINT,
  -- other columns...
  _ingest_ts TIMESTAMP
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

transformed = transformed.withColumn("_ingest_ts", F.current_timestamp())

transformed.createOrReplaceTempView("stage_incoming")

spark.sql(f"""
MERGE INTO {target_tbl} t
USING (SELECT * FROM stage_incoming) s
ON t.tenant = s.tenant AND t.id = s.id
WHEN MATCHED AND s.updated_at >= t.updated_at THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
""")

# 4) Advance state cautiously (set to the max obs updated_at in this batch)
if clean:
    import numpy as np
    by_tenant = (
        spark.table("stage_incoming")
        .groupBy("tenant").agg(F.max("updated_at").alias("max_ts"))
        .collect()
    )
    for r in by_tenant:
        spark.sql(f"""
        MERGE INTO {state_tbl} dst
        USING (SELECT '{r['tenant']}' AS tenant, {int(r['max_ts'])} AS last_ts) s
        ON dst.tenant = s.tenant
        WHEN MATCHED THEN UPDATE SET dst.last_ts = GREATEST(dst.last_ts, s.last_ts)
        WHEN NOT MATCHED THEN INSERT *
        """)

Notes
- If you have deletes, propagate a tombstone flag and handle it in MERGE (DELETE WHEN MATCHED AND s.is_deleted = true).
- Batch schedule: Databricks Job every 1‚Äì5 minutes is reasonable. Avoid per-minute if you expect large scans‚Äîtune to your RCU & batch size.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Option 2 ‚Äî DynamoDB Streams (CDC) ‚Üí S3 ‚Üí Databricks Auto Loader
Good for: ‚Äúhands-off‚Äù CDC with ordered updates per partition key (+ low ops).

Design
1) Enable **DynamoDB Streams** on the table with **New and old images** (or at least New image).
2) Wire up one of:
   A) **EventBridge Pipes**: Source= DynamoDB Streams ‚Üí Target= Kinesis Firehose (or S3) with optional input transform (Lambda or Enrichment).  
   B) **Lambda consumer** for Streams ‚Üí batch to S3 (Parquet/JSON) in small files (e.g., 5‚Äì50 MB).
   C) Streams ‚Üí **Kinesis Data Streams** (via adapter) ‚Üí **Kinesis Data Firehose** ‚Üí S3.

3) Land to **S3** with sensible partitioning: `s3://bucket/ddb/table_name/dt=YYYY-MM-dd/HH=HH/` and gzip/snappy parquet if you can.
4) **Databricks Auto Loader** reads the S3 path continuously and appends to a Delta table in Unity Catalog.
5) Do your PySpark transforms inline (Structured Streaming) and **MERGE for upserts** (using foreachBatch with MERGE).

Pros
- At-least-once CDC with built-in ordering per partition key from Streams.
- No cursor management in your code; retries handled by AWS service (Lambda/Firehose).
- Auto Loader handles schema inference/evolution + backpressure + exactly-once sink semantics (with Delta).

Cons & gotchas
- Slightly more AWS wiring (Streams + Pipes/Firehose or Lambda).
- You still need to MERGE to handle upserts (new images) & deletes (remove from Delta).

Auto Loader sketch (S3 JSON payloads with ‚ÄúImage‚Äù)
from pyspark.sql import functions as F

source_path = "s3://your-bucket/ddb/your-table/"
checkpoint = "s3://your-bucket/_checkpoints/your-table-stream"
schema_loc = "s3://your-bucket/_schemas/your-table-stream"

raw = (
  spark.readStream
       .format("cloudFiles")
       .option("cloudFiles.format", "json")
       .option("cloudFiles.inferColumnTypes", "true")
       .option("cloudFiles.schemaLocation", schema_loc)
       .load(source_path)
)

# Unwrap DDB stream record if needed (structure depends on your Lambda/Firehose transform)
# Example assumes payload like: {"eventName":"INSERT","dynamodb":{"NewImage": {...}, "Keys": {...}, "ApproximateCreationDateTime": 169...}}
df = raw  # apply select/transform to flatten NewImage ‚Üí columns

# Your business transform(s)
tx = (
  df
  # .withColumn(...)
  .withColumn("_ingest_ts", F.current_timestamp())
)

target_tbl = "hcai_databricks_dev.chargeminder2.fact_events"

def upsert_batch(micro_batch_df, batch_id: int):
    micro_batch_df.createOrReplaceTempView("incoming")
    spark.sql(f"""
    MERGE INTO {target_tbl} t
    USING (SELECT * FROM incoming) s
    ON t.tenant = s.tenant AND t.id = s.id
    WHEN MATCHED AND s.updated_at >= t.updated_at THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
    """)
    # Optional: handle DELETE records: issue DELETE WHERE pk IN (...) for those eventName='REMOVE'

(
  tx.writeStream
    .option("checkpointLocation", checkpoint)
    .trigger(processingTime="30 seconds")   # adjust: ‚ÄúavailableNow‚Äù for catch-ups or ‚Äúcontinuous‚Äù for low latency
    .foreachBatch(upsert_batch)
    .start()
)

Notes
- For deletions (`eventName='REMOVE'`), either land a tombstone record (is_deleted=true) in S3 and handle in MERGE, or run a post-upsert DELETE step for those keys inside `foreachBatch`.
- Use **DLT** (Delta Live Tables) if you want managed expectations/quality & lineage.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Option 3 ‚Äî DMS (full + CDC) ‚Üí S3 ‚Üí Auto Loader / DLT
- DMS supports DynamoDB ‚Üí S3 with CDC. Good for full backfills + ongoing changes with minimal code.
- Trade-offs: DMS operational overhead and cost; still end at S3 ‚Üí Delta with same downstream transform.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Option 4 ‚Äî Streams ‚Üí Kinesis Data Streams ‚Üí Databricks streaming (no S3)
- Lowest latency (seconds), but you‚Äôll need the Spark Kinesis connector + KCL adapter for DynamoDB Streams or a Lambda bridge to Kinesis.
- More moving parts; often not necessary unless strict real-time is required.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Security & credentials (Databricks on AWS)
- For **Serverless** compute, your workspace typically has a **Serverless Access Role**. To call DynamoDB directly (Option 1) from a notebook:
  - If your admin gave you **‚Äúchargeminder-dynamodb-creds‚Äù**, use `dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds")` with `boto3.Session(botocore_session=..., region_name=...)` exactly as in the sketch above.
  - Make sure that role grants: `dynamodb:DescribeTable`, `Query` (and/or `Scan` if used), and if Streams path: `dynamodb:DescribeStream`, `GetShardIterator`, `GetRecords`.
- For **S3 landing** (Options 2‚Äì3), prefer **Kinesis Firehose** or **Lambda** to write into an S3 prefix that your Unity Catalog external location / storage credential can read. Auto Loader needs `s3:GetObject`, `ListBucket` on that prefix.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Data modeling & correctness checklist
- **Primary key**: have a deterministic id (pk, sk) or a composite key you can use in MERGE.
- **Event time**: keep source `updated_at` and a `_ingest_ts`. Use watermarks if you do any time-based aggregates later.
- **Idempotency**: ALL sinks must be idempotent. Always MERGE on keys with `updated_at` guard.
- **Ordering**: Streams preserve order per partition key; batch pulls do not‚ÄîMERGE with a ‚Äúlatest-wins‚Äù rule.
- **Deletes**: Either soft-delete in DynamoDB and propagate a tombstone, or consume `REMOVE` from Streams and translate to Delta DELETE.
- **Schema evolution**: With Auto Loader, use `cloudFiles.schemaLocation` and set `spark.databricks.cloudFiles.schemaEvolutionMode` = ‚ÄúaddNewColumns‚Äù if needed.
- **Backfills**: For Option 2, drop historical exports in S3 under the same prefix and run Auto Loader with `availableNow`. For Option 1, rewind the cursor and reprocess (MERGE makes it safe).

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
What I recommend for you
Given your stack (Unity Catalog, PySpark, minute-ish SLAs, desire to keep ops light):
1) **Enable DynamoDB Streams** with New Images.  
2) **EventBridge Pipes ‚Üí Kinesis Firehose ‚Üí S3 (Parquet, snappy, 64‚Äì256 MB)**, partitioned by date/hour.  
3) **Databricks Auto Loader (Structured Streaming)** to transform and **MERGE** into `hcai_databricks_dev.chargeminder2.<your_table>`.  
4) Use a **DLT pipeline** if you want managed expectations and cleaner ops.  
5) Keep a tiny ‚Äúdead-letter‚Äù S3 prefix for records that fail transformation and alert on it.

If you need to start today and test quickly with small volumes, begin with **Option 1 (GSI pull)** using the sample code, then graduate to Streams ‚Üí S3 ‚Üí Auto Loader once you‚Äôre happy with the transform and table contract.

If you want, tell me your expected TPS, item size, tenants/partitioning model, and latency goal, and I‚Äôll size the RCU/Throughput, Firehose buffer hints, and Auto Loader trigger/checkpoint settings precisely.
```
