How to run your PySpark code every minute **outside a notebook** and manage it end-to-end.

## Step 1) Package your notebook logic as a Python entrypoint

Refactor your notebook into a tiny callable module plus a `main.py` runner. Put this in a Databricks Repo (recommended) or upload to DBFS.

```
repo-root/
  ingest/
    __init__.py
    job_logic.py      # your existing function(s)
    main.py           # entrypoint that calls your function
```

### File: job_logic.py (sketch)

```python
from pyspark.sql import SparkSession

def run_job():
    spark = SparkSession.builder.getOrCreate()

    # 1) Read from DynamoDB (however you currently do it)
    # ddb = spark.read.format("dynamodb")...  (or your connector)
    # 2) Transform
    # 3) Write to your Databricks table (on S3 / Delta)

    # example:
    # df.write.format("delta").mode("append").saveAsTable("my_catalog.my_schema.my_table")
```

### File main.py

```python
from ingest.job_logic import run_job

if __name__ == "__main__":
    run_job()
```

Push this repo to Git and use **Repos** in Databricks to keep it synced.

## Step 2) Create a Databricks Job (non-interactive)

You have three good compute choices:

* **Serverless compute for Jobs** (best if available): fully managed, quick starts, no cluster babysitting.
* **Job cluster**: ephemeral cluster created per run (simple, but cold-start overhead every minute can be costly).
* **Existing all-purpose cluster**: fastest per-minute cadence (no cold start), but you pay for it to stay running.

For a true **every-minute** cadence, prefer **Serverless** or an **Existing cluster kept warm**. A Job cluster spinning up every minute is usually too slow/expensive.

### Create the Job (UI)

* Workflows â†’ Jobs â†’ Create Job
* Task type: **Python script**
* Source: **Repo** â†’ pick your `main.py`
* Compute:

  * **Use Serverless compute** (if your workspace supports it), or
  * **Use existing cluster** (kept running; set Auto-Terminate to something long), or
  * **New job cluster** (only if you can tolerate startup time)
* Schedule: **Every 1 minute** (Scheduler uses UTC)
* Set **Timeout** (e.g., 50 seconds) and **Max concurrent runs = 1** to avoid overlaps
* Add **Retry** policy (e.g., 3 attempts, 10s backoff)
* Save

### Create the Job (JSON + CLI)

If you automate with the new `databricks` CLI:

### File: `job.json` (example; adjust paths)

```json
{
  "name": "ddb-to-delta-every-minute",
  "max_concurrent_runs": 1,
  "timeout_seconds": 50,
  "tasks": [
    {
      "task_key": "ingest-task",
      "python_wheel_task": null,
      "spark_python_task": {
        "python_file": "repos/you/your-repo/ingest/main.py"
      },
      "job_cluster_key": "job-cluster",
      "timeout_seconds": 50,
      "max_retries": 3,
      "min_retry_interval_millis": 10000,
      "retry_on_timeout": true
    }
  ],
  "job_clusters": [
    {
      "job_cluster_key": "job-cluster",
      "new_cluster": {
        "spark_version": "13.3.x-scala2.12",
        "node_type_id": "i3.xlarge",
        "num_workers": 1
      }
    }
  ],
  "schedule": {
    "quartz_cron_expression": "0 0/1 * * * ?",
    "timezone_id": "UTC",
    "pause_status": "UNPAUSED"
  }
}
```

### Create/update job :

```
databricks jobs create --json-file job.json
# or
databricks jobs reset --job-id <id> --json-file job.json
```

**Tip:** In the UI, selecting â€œEvery minuteâ€ will populate a correct Quartz cron; if youâ€™re unsure about the expression, use the UI first.

## Step 3) Credentials to DynamoDB/S3

Because youâ€™re on **Serverless/Jobs**, the clean path is an **instance profile / IAM role** that grants read to DynamoDB and write to your S3/Delta location. In the jobâ€™s compute settings:

* For **Serverless compute**: attach the workspaceâ€™s **serverless access**/instance profile (your admin might need to map it).
* For **Existing or Job clusters**: set the **Instance Profile / IAM role**.
  Then your Spark code can use the default provider chain (no hardcoded keys). In Python, `boto3` (if used) will pick up the role automatically.

## Step  4) If you truly need â€œevery minute,â€ consider streaming

A Structured Streaming job is often better than cron-style runs. You can:

* Wrap your logic as a streaming pipeline (e.g., `Trigger.ProcessingTime("1 minute")`).
* Deploy as a **continuous** job (no schedule; it runs once and keeps going).
* Use **Serverless** or an **existing cluster** to keep it alive.
  This avoids per-run startup, improves latency, and handles backpressure.

Minimal pattern:

```
(
  spark.readStream.format("...dynamodb-source...")
  .load()
  # transforms
  .writeStream
  .format("delta")
  .option("checkpointLocation", "s3://â€¦/chk/ingest-ddb")
  .trigger(processingTime="1 minute")
  .toTable("my_catalog.my_schema.my_table")
)
```

## Step 5) How to stop the non-interactive job

You have several options:

* **Cancel the active run:** Workflows â†’ Runs â†’ select run â†’ **Cancel**
  CLI: `databricks runs cancel --run-id <run-id>`
* **Pause the schedule:** Workflows â†’ Job â†’ **Pause**
  CLI: `databricks jobs pause --job-id <job-id>`
* **Delete the job:** Workflows â†’ Job â†’ **Delete**
  CLI: `databricks jobs delete --job-id <job-id>`
* **Stop compute:**

  * Serverless: stopping the run is enough; thereâ€™s no cluster to manage.
  * Existing cluster: **Terminate** it in Compute if you used one for the job.

## Step 6) Quick checklist for your case

* [ ] Move your notebook logic into `job_logic.run_job()`
* [ ] Put code in a Repo (so Jobs â†’ Python script can point to it)
* [ ] Choose **Serverless compute for Jobs** (or an existing warm cluster)
* [ ] Schedule â€œEvery minuteâ€ (UTC), timeout < 60s, retries, max concurrent = 1
* [ ] Ensure IAM role/instance profile grants DynamoDB read + S3 write
* [ ] (Optional but recommended) Convert to a single **streaming** job instead of per-minute batch

 
  **Databricks notebook scheduling (via the UI)** only supports hourly, daily, or weekly intervals â€” not per-minute execution.

# How to achieve **every-minute runs** :  several  patterns  
 

## ğŸ§­ 1. Use a Databricks **Job** with a cron schedule

Databricks **Jobs** (not the simple â€œScheduleâ€ in a notebook) can use **custom cron expressions**.

**Steps:**

1. Open **Workflows â†’ Jobs** in the left sidebar.
2. Click **Create Job**.
3. Under **Task**, select your notebook.
4. In **Schedule**, click **Advanced** and enter this cron expression:

   ```
   * * * * *    â†’ every minute
   ```
5. Save and run.

âœ… Runs every minute
âœ… Fully managed by Databricks
âœ… Viewable in the Job Runs history
âš ï¸ But: a new cluster may start for each run â€” use a **â€œContinuous jobâ€ cluster** or **existing job cluster** to minimize overhead.

---

## âš™ï¸ 2. Use **Databricks REST API** to trigger the job every minute

You can have an external scheduler (like Azure Function, AWS Lambda, or a local cron job) call:

```
POST /api/2.1/jobs/run-now
```

Example Python script:

```python
import time
import requests

DATABRICKS_INSTANCE = "https://<your-instance>.cloud.databricks.com"
JOB_ID = "<your-job-id>"
TOKEN = "<your-personal-access-token>"

while True:
    response = requests.post(
        f"{DATABRICKS_INSTANCE}/api/2.1/jobs/run-now",
        headers={"Authorization": f"Bearer {TOKEN}"},
        json={"job_id": JOB_ID}
    )
    print(response.json())
    time.sleep(60)  # wait 1 minute
```

âœ… Fine control over timing
âœ… Can handle retries or conditions
âš ï¸ Requires an always-on process somewhere

---

## ğŸ” 3. Loop inside the notebook itself

You can make your notebook run continuously, executing the code every minute inside a loop:

```python
import time

while True:
    print("Running task...")
    # your PySpark or Python logic here
    time.sleep(60)
```

âœ… Simplest approach
âš ï¸ Runs forever â€” must keep the notebook/cluster alive manually
âš ï¸ No job-level monitoring or retries

---

## ğŸª¶ 4. Use **Databricks Workflows â†’ Continuous jobs (Streaming mode)**

If your code reads from streaming sources or can be adapted to a long-running pattern, you can enable **continuous execution**.
Thatâ€™s ideal for incremental ingestion or minute-based aggregation.

In Job settings:

* Toggle **â€œRun continuouslyâ€**
* The code runs non-stop instead of restarting every minute.

âœ… Ideal for event processing or streaming data
âš ï¸ Not good for batch-style jobs that must start/finish each time

---

## ğŸ§° 5. Alternative external schedulers

If Databricks isnâ€™t strict about scheduling precision:

* Use **Azure Function (Timer Trigger)** every minute to call your Databricks job.
* Or use **Apache Airflow** DAG with `schedule_interval="@minutely"`.

---

### âœ… Summary

| Method                    | Granularity  | Recommended for       | Notes                 |
| ------------------------- | ------------ | --------------------- | --------------------- |
| Databricks Job (cron)     | Every minute | Standard automation   | Use `* * * * *`       |
| REST API loop             | Every minute | Custom orchestration  | Flexible, external    |
| Infinite loop in notebook | Every minute | Quick tests           | No retries            |
| Continuous job            | Real-time    | Streaming tasks       | Keeps cluster running |
| External scheduler        | Every minute | Integration pipelines | Use Function/Airflow  |

#  Problem statement
```
There is DynamoDB table populated by external process.
Goal is to build almost real-time pipeline which 
reads the new records from DynamoDB, transform it  using PySpark and append transformed records into DataBricks Unity catalog table.
```
I consider following options:

## Option 1:  Direct DynamoDB Table Access (Batch Reads)

Direct polling from DynamoDB table to Databricks using GSI global secondry index on column updated_at
Create Global Secondary Index (GSI) on updated_at column.
Periodically call table.query() 
Store max(updated_at) of already processed records in persistent storage, in order to read
only new recors:   max(updated_at) will be passed as argument to  tab.query() 


## Option 2: DynamoDB Streams (Change Data Capture)
 
AWS Lambda ->  S3 -> Databricks
 
### Comparison and  reference design
for a near-real-time (sub-minute to a few minutes)
DynamoDB â†’ PySpark â†’ Unity Catalog pipeline. 
 
```
- If you want â€œrobust + low-ops + ~1â€“2 min latencyâ€: **Use DynamoDB Streams â†’ (EventBridge Pipes or Kinesis Firehose) â†’ S3 â†’ Databricks Auto Loader â†’ Delta/Unity**. This is the sweet spot for cost, reliability, and simplicity.
- If volume is small and you need a quick start with minute polling: **Use a GSI on (tenant, updated_at)** and batch pull with a persisted cursor + idempotent MERGE into Delta.
- If you truly need sub-second to tens-of-seconds latency without an S3 landing zone: **Streams â†’ Kinesis Data Streams â†’ Databricks Structured Streaming**, but this adds complexity.
```

 
## Option 1 â€” Direct DynamoDB table access (batch pulls via GSI)  
Good for: low/medium volume, simple ops, minute-level cadence.

Design  
1) Model your GSI carefully:
   - **Partition key (HASH)**: something selective you can query per batch, e.g. tenant/account/org_id (or a fixed bucket key if global).
   - **Sort key (RANGE)**: updated_at (ISO-8601 or epoch millis).
   - If you have many tenants, youâ€™ll query per tenant; otherwise shard into a small set of buckets (e.g. pk = â€œbucket#00..99â€) so queries are bounded.

2) Pull pattern:  
   - Keep a persistent cursor per (partition key) = **last_processed_updated_at** in a tiny Delta â€œstateâ€ table.
   - Query GSI with `KeyConditionExpression: pk=:tenant AND updated_at BETWEEN :cursor AND :cursor + window`.
   - **Paginate** (1 MB limit/page) and **backoff** on throttling (RCU).
   - Allow **overlap** (e.g., re-read last 1â€“2 minutes) to tolerate clock skew & eventual consistency; dedupe with MERGE on primary keys.

3) Write pattern (idempotent):
   - Stage incoming micro-batch to a temp Delta table / view, then **MERGE** into your Unity table on (pk, sort_key) or a true unique id.

Pros
- Simple to reason about, no extra AWS infra required.
- Easy to backfill: just move the cursor back.

Cons & gotchas
- You must manage cursor, retries, and dedupe yourself.
- Hot partitions possible if updated_at is monotonically increasing and the GSI partition key is too coarse (watch RCU).
- Deletes and TTL expirations wonâ€™t be seen unless you model soft-deletes or do periodic reconciliations.

Code sketch (Databricks notebook, using your provided credential handle)
- Assumes your admin exposed AWS creds via `dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds")`.
- Replace `TENANT_IDS` with your partition keys or iterate all tenants you own.
```

```python
from pyspark.sql import functions as F

# 1) Read state (last processed updated_at per tenant)
 
state_tbl = "hcai_databricks_dev.chargeminder2.ddb_state"   # tenant, last_ts
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {state_tbl} (
  tenant STRING,
  last_ts BIGINT
) USING DELTA
""")

def get_boto3():
    import boto3
    sess = boto3.Session(
        botocore_session=dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds"),
        region_name="us-east-1"
    )
    return sess.resource("dynamodb"), sess.client("dynamodb")

table_name = "your_ddb_table"
gsi_name   = "tenant_updated_at_gsi"   # HASH = tenant, RANGE = updated_at

TENANT_IDS = ["tenantA","tenantB"]     # or discover/maintain this list elsewhere
MAX_LAG_MS = 120000                    # re-read last 2 minutes for overlap

ddb_resource, ddb_client = get_boto3()
tbl = ddb_resource.Table(table_name)

import time, decimal
now_ms = int(time.time() * 1000)

rows = []
for tenant in TENANT_IDS:
    last = spark.table(state_tbl).where(F.col("tenant")==tenant).limit(1).collect()
    last_ts = last[0]["last_ts"] if last else (now_ms - 3600_000)  # default: 1 hour back for first run
    start_ts = max(0, last_ts - MAX_LAG_MS)
    kwargs = {
        "IndexName": gsi_name,
        "KeyConditionExpression": "#t = :tenant AND #u BETWEEN :from AND :to",
        "ExpressionAttributeNames": {"#t":"tenant", "#u":"updated_at"},
        "ExpressionAttributeValues": {":tenant": tenant, ":from": start_ts, ":to": now_ms},
        "Limit": 1000
    }
    while True:
        resp = tbl.query(**kwargs)
        for item in resp.get("Items", []):
            rows.append(item)
        if "LastEvaluatedKey" not in resp:
            break
        kwargs["ExclusiveStartKey"] = resp["LastEvaluatedKey"]

# Convert to Spark (handle Decimal)
def to_native(o):
    import decimal
    if isinstance(o, decimal.Decimal):
        return float(o)
    if isinstance(o, dict):
        return {k: to_native(v) for k,v in o.items()}
    if isinstance(o, list):
        return [to_native(v) for v in o]
    return o

clean = [to_native(r) for r in rows]
df = spark.createDataFrame(clean) if clean else spark.createDataFrame([], "tenant STRING")  # empty safe

# 2) Transform with PySpark
transformed = (
    df
    # .withColumn(...)  # your business transforms
)

# 3) Upsert into UC Delta table
target_tbl = "hcai_databricks_dev.chargeminder2.fact_events"

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {target_tbl} (
  tenant STRING,
  id STRING,
  updated_at BIGINT,
  -- other columns...
  _ingest_ts TIMESTAMP
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

transformed = transformed.withColumn("_ingest_ts", F.current_timestamp())

transformed.createOrReplaceTempView("stage_incoming")

spark.sql(f"""
MERGE INTO {target_tbl} t
USING (SELECT * FROM stage_incoming) s
ON t.tenant = s.tenant AND t.id = s.id
WHEN MATCHED AND s.updated_at >= t.updated_at THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
""")

# 4) Advance state cautiously (set to the max obs updated_at in this batch)

if clean:
    import numpy as np
    by_tenant = (
        spark.table("stage_incoming")
        .groupBy("tenant").agg(F.max("updated_at").alias("max_ts"))
        .collect()
    )
    for r in by_tenant:
        spark.sql(f"""
        MERGE INTO {state_tbl} dst
        USING (SELECT '{r['tenant']}' AS tenant, {int(r['max_ts'])} AS last_ts) s
        ON dst.tenant = s.tenant
        WHEN MATCHED THEN UPDATE SET dst.last_ts = GREATEST(dst.last_ts, s.last_ts)
        WHEN NOT MATCHED THEN INSERT *
        """)
```
Notes
- If you have deletes, propagate a tombstone flag and handle it in MERGE (DELETE WHEN MATCHED AND s.is_deleted = true).
- Batch schedule: Databricks Job every 1â€“5 minutes is reasonable. Avoid per-minute if you expect large scansâ€”tune to your RCU & batch size.


## Option 2 â€” DynamoDB Streams (CDC) â†’ S3 â†’ Databricks Auto Loader
Good for: â€œhands-offâ€ CDC with ordered updates per partition key (+ low ops).

Design
1) Enable **DynamoDB Streams** on the table with **New and old images** (or at least New image).
2) Wire up one of:
   A) **EventBridge Pipes**: Source= DynamoDB Streams â†’ Target= Kinesis Firehose (or S3) with optional input transform (Lambda or Enrichment).  
   B) **Lambda consumer** for Streams â†’ batch to S3 (Parquet/JSON) in small files (e.g., 5â€“50 MB).
   C) Streams â†’ **Kinesis Data Streams** (via adapter) â†’ **Kinesis Data Firehose** â†’ S3.

3) Land to **S3** with sensible partitioning: `s3://bucket/ddb/table_name/dt=YYYY-MM-dd/HH=HH/` and gzip/snappy parquet if you can.
4) **Databricks Auto Loader** reads the S3 path continuously and appends to a Delta table in Unity Catalog.
5) Do your PySpark transforms inline (Structured Streaming) and **MERGE for upserts** (using foreachBatch with MERGE).

Pros
- At-least-once CDC with built-in ordering per partition key from Streams.
- No cursor management in your code; retries handled by AWS service (Lambda/Firehose).
- Auto Loader handles schema inference/evolution + backpressure + exactly-once sink semantics (with Delta).

Cons & gotchas
- Slightly more AWS wiring (Streams + Pipes/Firehose or Lambda).
- You still need to MERGE to handle upserts (new images) & deletes (remove from Delta).

Auto Loader sketch (S3 JSON payloads with â€œImageâ€)

```python
from pyspark.sql import functions as F

source_path = "s3://your-bucket/ddb/your-table/"
checkpoint = "s3://your-bucket/_checkpoints/your-table-stream"
schema_loc = "s3://your-bucket/_schemas/your-table-stream"

raw = (
  spark.readStream
       .format("cloudFiles")
       .option("cloudFiles.format", "json")
       .option("cloudFiles.inferColumnTypes", "true")
       .option("cloudFiles.schemaLocation", schema_loc)
       .load(source_path)
)

# Unwrap DDB stream record if needed (structure depends on your Lambda/Firehose transform)
# Example assumes payload like: {"eventName":"INSERT","dynamodb":{"NewImage": {...}, "Keys": {...}, "ApproximateCreationDateTime": 169...}}
df = raw  # apply select/transform to flatten NewImage â†’ columns

# Your business transform(s)
tx = (
  df
  # .withColumn(...)
  .withColumn("_ingest_ts", F.current_timestamp())
)

target_tbl = "hcai_databricks_dev.chargeminder2.fact_events"

def upsert_batch(micro_batch_df, batch_id: int):
    micro_batch_df.createOrReplaceTempView("incoming")
    spark.sql(f"""
    MERGE INTO {target_tbl} t
    USING (SELECT * FROM incoming) s
    ON t.tenant = s.tenant AND t.id = s.id
    WHEN MATCHED AND s.updated_at >= t.updated_at THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
    """)
    # Optional: handle DELETE records: issue DELETE WHERE pk IN (...) for those eventName='REMOVE'

(
  tx.writeStream
    .option("checkpointLocation", checkpoint)
    .trigger(processingTime="30 seconds")   # adjust: â€œavailableNowâ€ for catch-ups or â€œcontinuousâ€ for low latency
    .foreachBatch(upsert_batch)
    .start()
)
```
### Notes

```
- For deletions (`eventName='REMOVE'`), either land a tombstone record (is_deleted=true) in S3 and handle in MERGE, or run a post-upsert DELETE step for those keys inside `foreachBatch`.
- Use **DLT** (Delta Live Tables) if you want managed expectations/quality & lineage.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option 3 â€” DMS (full + CDC) â†’ S3 â†’ Auto Loader / DLT
- DMS supports DynamoDB â†’ S3 with CDC. Good for full backfills + ongoing changes with minimal code.
- Trade-offs: DMS operational overhead and cost; still end at S3 â†’ Delta with same downstream transform.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Option 4 â€” Streams â†’ Kinesis Data Streams â†’ Databricks streaming (no S3)
- Lowest latency (seconds), but youâ€™ll need the Spark Kinesis connector + KCL adapter for DynamoDB Streams or a Lambda bridge to Kinesis.
- More moving parts; often not necessary unless strict real-time is required.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Security & credentials (Databricks on AWS)
- For **Serverless** compute, your workspace typically has a **Serverless Access Role**. To call DynamoDB directly (Option 1) from a notebook:
  - If your admin gave you **â€œchargeminder-dynamodb-credsâ€**, use `dbutils.credentials.getServiceCredentialsProvider("chargeminder-dynamodb-creds")` with `boto3.Session(botocore_session=..., region_name=...)` exactly as in the sketch above.
  - Make sure that role grants: `dynamodb:DescribeTable`, `Query` (and/or `Scan` if used), and if Streams path: `dynamodb:DescribeStream`, `GetShardIterator`, `GetRecords`.
- For **S3 landing** (Options 2â€“3), prefer **Kinesis Firehose** or **Lambda** to write into an S3 prefix that your Unity Catalog external location / storage credential can read. Auto Loader needs `s3:GetObject`, `ListBucket` on that prefix.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Data modeling & correctness checklist
- **Primary key**: have a deterministic id (pk, sk) or a composite key you can use in MERGE.
- **Event time**: keep source `updated_at` and a `_ingest_ts`. Use watermarks if you do any time-based aggregates later.
- **Idempotency**: ALL sinks must be idempotent. Always MERGE on keys with `updated_at` guard.
- **Ordering**: Streams preserve order per partition key; batch pulls do notâ€”MERGE with a â€œlatest-winsâ€ rule.
- **Deletes**: Either soft-delete in DynamoDB and propagate a tombstone, or consume `REMOVE` from Streams and translate to Delta DELETE.
- **Schema evolution**: With Auto Loader, use `cloudFiles.schemaLocation` and set `spark.databricks.cloudFiles.schemaEvolutionMode` = â€œaddNewColumnsâ€ if needed.
- **Backfills**: For Option 2, drop historical exports in S3 under the same prefix and run Auto Loader with `availableNow`. For Option 1, rewind the cursor and reprocess (MERGE makes it safe).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
What I recommend for you
Given your stack (Unity Catalog, PySpark, minute-ish SLAs, desire to keep ops light):
1) **Enable DynamoDB Streams** with New Images.  
2) **EventBridge Pipes â†’ Kinesis Firehose â†’ S3 (Parquet, snappy, 64â€“256 MB)**, partitioned by date/hour.  
3) **Databricks Auto Loader (Structured Streaming)** to transform and **MERGE** into `hcai_databricks_dev.chargeminder2.<your_table>`.  
4) Use a **DLT pipeline** if you want managed expectations and cleaner ops.  
5) Keep a tiny â€œdead-letterâ€ S3 prefix for records that fail transformation and alert on it.

If you need to start today and test quickly with small volumes, begin with **Option 1 (GSI pull)** using the sample code, then graduate to Streams â†’ S3 â†’ Auto Loader once youâ€™re happy with the transform and table contract.

If you want, tell me your expected TPS, item size, tenants/partitioning model, and latency goal, and Iâ€™ll size the RCU/Throughput, Firehose buffer hints, and Auto Loader trigger/checkpoint settings precisely.
```


## 2ï¸âƒ£ How the pipeline avoids reprocessing already processed JSONL files

This is handled automatically by **Auto Loader** using its **checkpoint** mechanism and internal **file notification logs**.

### Key configs in your code:

```python
.option("cloudFiles.format", "json")
.option("cloudFiles.schemaLocation", SCHEMA_LOC)
.option("checkpointLocation", CHECKPOINT)
```

Hereâ€™s what happens behind the scenes:

| Component                               | Path                                              | Purpose                                                                                                                       |
| --------------------------------------- | ------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **`schemaLocation`** (`SCHEMA_LOC`)     | `s3://chargeminder-2/_schemas/fact_telemetry`     | Auto Loader stores its **inferred schema** and evolution metadata. Itâ€™s used to track JSON field types and changes over time. |
| **`checkpointLocation`** (`CHECKPOINT`) | `s3://chargeminder-2/_checkpoints/fact_telemetry` | Delta Streaming engine stores **progress state**, offsets, and a list of files already processed. This ensures idempotency.   |

### How Auto Loader tracks files:

* On first run, it lists or subscribes to S3 events.
* For each file found, it records a unique **file ID and modification timestamp** in the checkpoint metadata.
* On subsequent runs, Auto Loader queries S3 and **skips any file already recorded** in the checkpoint.
* If a file is modified or re-uploaded with the same name, Auto Loader still ignores it because its internal file ID (ETag) hasnâ€™t changed â€” unless you explicitly delete the checkpoint.

So:

> âœ… Even if your Lambda keeps writing new `batch_YYYY-MM-DD-HH-MM-SS_N.jsonl` files into S3,
> only new files will be picked up by Databricks.

If you ever **delete the checkpoint folder**, the pipeline will reprocess **everything** in that S3 path.

---

## 3ï¸âƒ£ Purpose of `"schema_path": "s3://chargeminder-2/_schemas/fact_telemetry"`

This directory has **nothing to do with incremental tracking**.
It is purely for **schema inference + evolution tracking** for Auto Loader.

### Specifically:

* On the first read, Auto Loader inspects your JSONL files and infers schema.
* It saves the schema as a **Delta JSON file** in the `_schemas/fact_telemetry` directory.
* Next time you restart the stream, it reuses that schema â€” so you donâ€™t have to infer it again.
* If new fields appear later (say, your Lambda starts adding `battery_level`), Auto Loader detects them and **updates** the schema JSON in `_schemas/â€¦`.

> This prevents repeated full scans of historical data and avoids schema drift errors between batches.

---

## 4ï¸âƒ£ TL;DR summary

| Topic                                          | What It Does                                                      |
| ---------------------------------------------- | ----------------------------------------------------------------- |
| `spark.databricks.delta.optimizeWrite.enabled` | Optional optimization toggle; safe to remove.                     |
| `checkpointLocation`                           | Tracks already processed files and offsets â†’ prevents duplicates. |
| `schemaLocation`                               | Stores JSON schema evolution â†’ prevents re-inference.             |
| Reprocessing old files                         | Prevented by checkpoint; files are logged as â€œseenâ€.              |
| Deleting checkpoint                            | Reprocesses **all** files in S3 prefix.                           |

---

Short answer: **yes**, that will workâ€”just do it cleanly and in the right order.

#  **Reprocess all existing .jsonl files**:

1. **Stop any running stream**

* Make sure no active query is using that checkpoint.
* In notebooks: `q.stop()` if you kept the handle, or use the UI to stop the stream.

2. **Clear the target table**

```sql
USE CATALOG hcai_databricks_dev;
USE SCHEMA chargeminder2;
TRUNCATE TABLE fact_telemetry;
```

(If you also want to remove history to keep the Delta log tiny: `DELETE FROM fact_telemetry; VACUUM fact_telemetry RETAIN 0 HOURS;` â€” only if you know what youâ€™re doing.)

3. **Reset the checkpoint (recommended: delete the whole directory)**

```python
dbutils.fs.rm("s3://chargeminder-2/_checkpoints/fact_telemetry", recurse=True)
```

* Deleting only `commits/`, `offsets/`, `sources/` is usually fine, but wiping the **entire** checkpoint folder avoids stray state files.
* Alternatively, point your job to a **new checkpoint path** (e.g., add a datestamp suffix). This is the least error-prone.

4. **Leave `schema_path` alone**

* You can keep `s3://chargeminder-2/_schemas/fact_telemetry` as-is. Itâ€™s for schema evolution metadata and doesnâ€™t control reprocessing.
* If you provided an explicit schema in code, that folder may stay emptyâ€”totally fine.

5. **Run your Python stream again**

* With an **empty/nonexistent checkpoint**, Auto Loader treats the run as first-time and will pick up **all existing files** under your `s3://chargeminder-2/raw/dynamodb/chargeminder-car-telemetry` prefix.
* Using `availableNow` trigger is perfect for a one-shot backfill:

```python
writer = writer.trigger(availableNow=True)
q = writer.start()
q.awaitTermination()
```

Notes / gotchas:

* Make sure your source files are still present in S3 (not expired/overwritten).
* If your Lambda is still writing new files while you backfill, youâ€™ll ingest **both** old and newâ€”usually fine, but just be aware.
* You do **not** need to touch `schema_path` to force reprocessing. That path doesnâ€™t control file progress.
* If you want to reprocess **only a subset**, temporarily change `SOURCE_PATH` to a narrower S3 prefix, run `availableNow`, then switch back.

Optional quick sanity checks before/after:

```python
# How many source files do we see?
print(spark.read.format("json").load(SOURCE_PATH).count())

# Checkpoint exists?
display(dbutils.fs.ls("s3://chargeminder-2/_checkpoints/"))
```

If you want, I can give you a tiny cell that flips your job to a **new checkpoint path** automatically (e.g., `_checkpoints/fact_telemetry_reprocess_2025-11-04`) so you donâ€™t have to delete anything.


### âœ… Recommendation

Keep both schema and checkpoint paths persistent and unique per stream:

```python
"schema_path": "s3://chargeminder-2/_schemas/fact_telemetry",
"checkpoint_path": "s3://chargeminder-2/_checkpoints/fact_telemetry"
```

That ensures your stream is:

* **Incremental** (processes only new batches),
* **Idempotent** (no duplicates),
* **Schema-aware** (adapts to new columns).

 The content type `application/x-ndjson` is actually correct for NDJSON format - the mismatch with `.jsonl` extension is just a naming convention thing (both `.jsonl` and `.ndjson` are valid). However, let's make this Lambda **much more robust** for production use:Perfect! Here's everything you need to make your Lambda production-ready:

## ğŸ“¥ Download Links

### Production-Ready Lambda Files
- **[lambda_robust.py](computer:///mnt/user-data/outputs/lambda_robust.py)** - Production Lambda with all improvements
- **[deploy_robust.sh](computer:///mnt/user-data/outputs/deploy_robust.sh)** - Automated deployment script
- **[LAMBDA_IMPROVEMENTS.md](computer:///mnt/user-data/outputs/LAMBDA_IMPROVEMENTS.md)** - Detailed documentation
- **[LAMBDA_COMPARISON.md](computer:///mnt/user-data/outputs/LAMBDA_COMPARISON.md)** - Side-by-side comparison

---

## âœ… Your Questions Answered

### 1. Content Type vs File Extension

**Your Question:** Why `.jsonl` extension with `application/x-ndjson` content type?

**Answer:** Both are correct! 
- `.jsonl` and `.ndjson` are both valid extensions for newline-delimited JSON
- `application/x-ndjson` is the standard MIME type
- **For consistency**, the robust version uses `.ndjson` extension to match the content type

### 2. How to Make Lambda More Robust

Your basic Lambda has **5 critical issues**:

| Issue | Impact | Solution |
|-------|--------|----------|
| âŒ **No error handling** | One bad record crashes everything | âœ… Try-catch per record |
| âŒ **No retry logic** | Transient S3 failures are permanent | âœ… 3 retries with backoff |
| âŒ **No validation** | Bad data goes to S3 | âœ… Validate required fields |
| âŒ **No monitoring** | Can't track failures | âœ… CloudWatch metrics |
| âŒ **No DLQ** | Failed records are lost forever | âœ… Dead Letter Queue |

---

## ğŸ¯ What Gets Added

### **10 Production Features:**

1. âœ… **Per-record error handling** - Bad records don't crash the batch
2. âœ… **Retry logic** - 3 attempts with exponential backoff (1s, 2s, 4s)
3. âœ… **Data validation** - Checks required fields before processing
4. âœ… **Dead Letter Queue** - Failed records go to SQS for recovery
5. âœ… **Partial batch failures** - Only retry failed records (AWS best practice)
6. âœ… **CloudWatch metrics** - 5 custom metrics for monitoring
7. âœ… **Processing metadata** - Track when/how records were processed
8. âœ… **S3 metadata** - Rich object metadata for debugging
9. âœ… **Type hints** - Better code quality and IDE support
10. âœ… **Structured logging** - Clear, searchable logs

---

## ğŸ“Š Comparison Table

| Feature | Your Lambda | Robust Lambda |
|---------|-------------|---------------|
| Lines of code | ~70 | ~320 |
| Error handling | âŒ None | âœ… Comprehensive |
| Retry logic | âŒ No | âœ… 3 attempts |
| Validation | âŒ No | âœ… Yes |
| Monitoring | âŒ No | âœ… 5 metrics |
| DLQ support | âŒ No | âœ… Yes |
| Partial failures | âŒ No | âœ… Yes |
| Cost/month (1M records) | $1.00 | $1.50 |
| Production-ready | âŒ No | âœ… Yes |

**Cost increase: $0.50/month for huge reliability gains!**

---

## ğŸš€ Quick Deployment

```bash
# 1. Download files
# Download lambda_robust.py and deploy_robust.sh

# 2. Update configuration in deploy_robust.sh
nano deploy_robust.sh
# Set: ROLE_ARN, S3_BUCKET, DYNAMODB_TABLE

# 3. Deploy everything (10 minutes)
chmod +x deploy_robust.sh
./deploy_robust.sh
```

**The script automatically sets up:**
- Lambda function with new code
- Dead Letter Queue (SQS)
- IAM permissions (CloudWatch, SQS)
- DynamoDB Stream trigger with partial batch failure support
- CloudWatch alarms (errors, failed records, S3 failures)
- SNS topic for alerts

---

## ğŸ” Key Improvements Explained

### **1. Error Handling**
```python
# Before: Crashes on any error
obj = {k: deser.deserialize(v) for k, v in new_image.items()}

# After: Catches errors per record
try:
    obj = {k: deser.deserialize(v) for k, v in new_image.items()}
    if not validate_record(obj):
        raise ProcessingError("Validation failed")
except Exception as e:
    send_to_dlq(record, str(e))
    continue  # Process remaining records
```

### **2. Partial Batch Failures**
```python
# Tell DynamoDB Streams which records to retry
return {
    "batchItemFailures": [
        {"itemIdentifier": "event-id-123"},  # Only retry this one
        {"itemIdentifier": "event-id-456"}   # And this one
    ]
}
```

**Benefit:** Prevents reprocessing successful records!

### **3. Retry Logic**
```python
def write_to_s3(lines, key, attempt=1):
    try:
        s3_client.put_object(...)
    except Exception as e:
        if attempt < 3:
            time.sleep(2 ** attempt)  # 1s, 2s, 4s
            return write_to_s3(lines, key, attempt + 1)
```

### **4. CloudWatch Metrics**
```python
publish_metric("RecordsProcessed", 100)
publish_metric("RecordsFailed", 2)
publish_metric("ProcessingDuration", 1.5, "Seconds")
```

---

## ğŸ“ˆ Monitoring

### CloudWatch Metrics Available
- `RecordsProcessed` - Successfully processed
- `RecordsFailed` - Failed validation/processing
- `RecordsSkipped` - Skipped (wrong event type)
- `ProcessingDuration` - Lambda execution time
- `S3WriteSuccess` - Successful S3 writes
- `S3WriteFailure` - Failed S3 writes

### CloudWatch Insights Queries

**Failed Records:**
```sql
fields @timestamp, @message
| filter @message like /Failed to process record/
| sort @timestamp desc
```

**Processing Summary:**
```sql
fields @timestamp, @message
| filter @message like /Processing Summary/
| parse @message /Processed: (?<processed>\d+)/
| stats sum(processed) by bin(5m)
```

---

## ğŸ“ Migration Guide

### Option 1: Direct Update (Recommended)
```bash
# Just run the deployment script
./deploy_robust.sh
# Your function is immediately updated
```

### Option 2: Test First
```bash
# Deploy as new function
FUNCTION_NAME="chargeminder-stream-processor-v2" ./deploy_robust.sh

# Test with sample data
# Switch DynamoDB Stream trigger to new function
# Delete old function
```

---

## âœ… After Deployment

### 1. Subscribe to Alerts
```bash
aws sns subscribe \
    --topic-arn arn:aws:sns:region:account:chargeminder-lambda-alerts \
    --protocol email \
    --notification-endpoint your-email@example.com
```

### 2. Test the Function
```python
# Insert test record into DynamoDB
import boto3
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('chargeminder-car-telemetry')
table.put_item(Item={'event_id': 'test-123', 'recorded_at': '2025-10-31 12:00:00'})
```

### 3. Monitor
```bash
# Check logs
aws logs tail /aws/lambda/chargeminder-stream-processor --follow

# Check metrics
aws cloudwatch get-metric-statistics \
    --namespace ChargeMinder/Lambda \
    --metric-name RecordsProcessed \
    --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
    --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
    --period 300 \
    --statistics Sum

# Check DLQ (if any failures)
aws sqs receive-message --queue-url YOUR_DLQ_URL
```


## ğŸ’¡ Bottom Line

**Your Lambda works**, but it's not production-ready. The robust version adds:

1. **Reliability** - Handles errors gracefully
2. **Observability** - Know what's happening
3. **Recoverability** - Failed records go to DLQ
4. **Best Practices** - Follows AWS recommendations

**Cost:** Only +$0.50/month for 1M records

**Recommendation:** Use the robust version for production! ğŸš€
